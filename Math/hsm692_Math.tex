\documentclass[12pt]{article} 

\usepackage[utf8]{inputenc}
\usepackage{geometry} 
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage[parfill]{parskip}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float

\usepackage{amsmath}
\usepackage{amssymb}

\geometry{letterpaper}

\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\rmfamily\mdseries\upshape} % (See the fntguide.pdf for font help)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...
\pagenumbering{arabic}

\graphicspath{{./images/}}

\title{Implementing a Convolutional Neural Network for Malaria diagnosis}
\author{Personal code: hms692}
\date{First Submission: December 2021}

\begin{document}
\maketitle

\section{Topic}

\section{Personal Engagement}

\section{Background}
\includegraphics[width=\textwidth]{Perceptron}

This diagram shows the basic form of the perceptron, the building block of a neural network. It takes an input vector, transforms it by a set of random weights, and then adds random biases. This linear combination $\Sigma x_i w_i + b$ is then processed by an activation function (usually the sigmoid function $\sigma(x) = \frac{1}{1 + e^{-x}}$). This is then taken as the output of the perceptron which can be fed as the input of one or more new perceptrons, each with their own set of random weights and biases. This is the feed-forward processing phase. Because all of the weights and biases are random in this first phase, the final output of the model is nonsense. 

In order to tune the weights and biases, the backpropgagation algorithm is used. We introduce an error function $E = \frac{1}{2}(\vec{y} - out)^2$ where $\vec{y}$ is the known correct output of the model. "Solving" the model corresponds to minimising this error function: 

\section{Solution}

\[
\begin{bmatrix}
    1 & 2 & 3\\
    4 & 5 & 6\\
    7 & 8 & 9
\end{bmatrix} * \begin{bmatrix}
    1 & 1\\
    -1 & -1
\end{bmatrix} \rightarrow \begin{bmatrix}
   1 & 2 & 4 & 5\\ 
   2 & 3 & 5 & 6\\
   4 & 5 & 7 & 8\\
   5 & 6 & 8 & 9
\end{bmatrix} \cdot \begin{bmatrix}
    1\\
    1\\
    -1\\
    -1
\end{bmatrix} = \begin{bmatrix}
    -6\\
    -6\\
    -6\\
    -6
\end{bmatrix} \rightarrow \begin{bmatrix}
    -6 & -6\\
    -6 & -6
\end{bmatrix}
\]

\end{document}